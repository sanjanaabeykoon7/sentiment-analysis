{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c7ab7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Sentiment Analysis on Movie Reviews\n",
    "\n",
    "**Objective**: Build a machine learning model to classify movie reviews as positive or negative.\n",
    "\n",
    "**Dataset**: IMDB Movie Reviews Dataset (50,000 reviews)\n",
    "\n",
    "**Tools**: Python, scikit-learn, NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB Dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\n‚úì Dataset loaded successfully!\")\n",
    "print(f\"\\nFirst few reviews:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['sentiment'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Distribution of Sentiments', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['review']\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['review']\n",
    "\n",
    "# Combine all text for each sentiment\n",
    "positive_text = ' '.join(positive_reviews)\n",
    "negative_text = ' '.join(negative_reviews)\n",
    "\n",
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Positive word cloud\n",
    "wordcloud_pos = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          colormap='Greens',\n",
    "                          max_words=100).generate(positive_text)\n",
    "\n",
    "axes[0].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "axes[0].set_title('Most Common Words in Positive Reviews', fontsize=16, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative word cloud\n",
    "wordcloud_neg = WordCloud(width=800, height=400,\n",
    "                          background_color='white', \n",
    "                          colormap='Reds',\n",
    "                          max_words=100).generate(negative_text)\n",
    "\n",
    "axes[1].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "axes[1].set_title('Most Common Words in Negative Reviews', fontsize=16, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Word clouds generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e506ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add review length column\n",
    "df['review_length'] = df['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Statistics\n",
    "print(\"Review Length Statistics:\")\n",
    "print(df.groupby('sentiment')['review_length'].describe())\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "df.boxplot(column='review_length', by='sentiment', figsize=(10, 6))\n",
    "plt.title('Review Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we're working with\n",
    "print(\"Sample raw reviews BEFORE cleaning:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(df['review'].iloc[i][:200] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords if not already done\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test the function on one review\n",
    "print(\"BEFORE cleaning:\")\n",
    "print(df['review'].iloc[0][:300])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"AFTER cleaning:\")\n",
    "print(clean_text(df['review'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaaa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all reviews (this takes ~30 seconds)\n",
    "print(\"Cleaning all 50,000 reviews... ‚è≥\")\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "print(\"‚úì Cleaning complete!\")\n",
    "print(f\"\\nDataset now has {len(df.columns)} columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few cleaned reviews:\")\n",
    "df[['review', 'cleaned_review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20542b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show side-by-side comparison\n",
    "print(\"COMPARISON - Original vs Cleaned:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"ORIGINAL ({len(df['review'].iloc[i])} chars):\")\n",
    "    print(df['review'].iloc[i][:200] + \"...\")\n",
    "    print(f\"\\nCLEANED ({len(df['cleaned_review'].iloc[i])} chars):\")\n",
    "    print(df['cleaned_review'].iloc[i][:200])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and labels (y)\n",
    "X = df['cleaned_review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining sentiment distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features using TF-IDF\n",
    "print(\"Converting text to TF-IDF features... ‚è≥\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Keep top 5000 most important words\n",
    "    min_df=5,           # Word must appear in at least 5 documents\n",
    "    max_df=0.7,         # Ignore words appearing in more than 70% of documents\n",
    "    ngram_range=(1, 2)  # Use single words and pairs of words\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"‚úì TF-IDF conversion complete!\")\n",
    "print(f\"\\nFeature matrix shape:\")\n",
    "print(f\"Training: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing: {X_test_tfidf.shape}\")\n",
    "print(f\"\\nThis means: {X_train_tfidf.shape[0]} reviews √ó {X_train_tfidf.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb307679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Total vocabulary size: {len(feature_names)}\")\n",
    "print(f\"\\nSample features (words/phrases):\")\n",
    "print(feature_names[:20])\n",
    "print(\"\\n...\")\n",
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "print(\"Training Logistic Regression model... ‚è≥\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Training complete in {training_time:.2f} seconds!\")\n",
    "print(f\"\\nLogistic Regression Accuracy: {accuracy_score(y_test, y_pred_log):.4f}\")\n",
    "print(f\"That's {accuracy_score(y_test, y_pred_log)*100:.2f}% accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION - DETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_log, target_names=['negative', 'positive']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_log = confusion_matrix(y_test, y_pred_log)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_log)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['negative', 'positive'],\n",
    "            yticklabels=['negative', 'positive'])\n",
    "plt.title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"Training Naive Bayes model... ‚è≥\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Training complete in {training_time:.2f} seconds!\")\n",
    "print(f\"\\nNaive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"That's {accuracy_score(y_test, y_pred_nb)*100:.2f}% accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65448781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NAIVE BAYES - DETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=['negative', 'positive']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_nb)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['negative', 'positive'],\n",
    "            yticklabels=['negative', 'positive'])\n",
    "plt.title('Naive Bayes - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_log),\n",
    "        accuracy_score(y_test, y_pred_nb)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(results['Model'], results['Accuracy'], color=['#3498db', '#2ecc71'])\n",
    "plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)  # Focus on the relevant range\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.3, label='90% baseline')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}\\n({height*100:.2f}%)',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = results.loc[results['Accuracy'].idxmax(), 'Model']\n",
    "best_accuracy = results['Accuracy'].max()\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} with {best_accuracy*100:.2f}% accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda90410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our best model on some custom reviews!\n",
    "def predict_sentiment(text, model=log_reg):\n",
    "    \"\"\"\n",
    "    Predict sentiment of a given text\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    cleaned = clean_text(text)\n",
    "    # Transform using TF-IDF\n",
    "    text_tfidf = tfidf.transform([cleaned])\n",
    "    # Predict\n",
    "    prediction = model.predict(text_tfidf)[0]\n",
    "    probability = model.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! Best film I've seen this year!\",\n",
    "    \"Terrible movie. Complete waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"Amazing acting and brilliant storyline. Highly recommend!\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING ON CUSTOM REVIEWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    sentiment, prob = predict_sentiment(review)\n",
    "    confidence = prob[1] if sentiment == 'positive' else prob[0]\n",
    "    \n",
    "    print(f\"\\nReview {i}: \\\"{review}\\\"\")\n",
    "    print(f\"Predicted: {sentiment.upper()} (Confidence: {confidence*100:.2f}%)\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461de95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most important words for each sentiment\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "# Get coefficients from Logistic Regression\n",
    "coefficients = log_reg.coef_[0]\n",
    "\n",
    "# Top 20 words for positive sentiment\n",
    "top_positive_indices = coefficients.argsort()[-20:][::-1]\n",
    "top_positive_words = feature_names[top_positive_indices]\n",
    "top_positive_scores = coefficients[top_positive_indices]\n",
    "\n",
    "# Top 20 words for negative sentiment\n",
    "top_negative_indices = coefficients.argsort()[:20]\n",
    "top_negative_words = feature_names[top_negative_indices]\n",
    "top_negative_scores = coefficients[top_negative_indices]\n",
    "\n",
    "# Display\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 20 WORDS INDICATING POSITIVE SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "for word, score in zip(top_positive_words, top_positive_scores):\n",
    "    print(f\"{word:20s} | {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 WORDS INDICATING NEGATIVE SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "for word, score in zip(top_negative_words, top_negative_scores):\n",
    "    print(f\"{word:20s} | {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Positive words\n",
    "axes[0].barh(range(15), top_positive_scores[:15], color='green', alpha=0.7)\n",
    "axes[0].set_yticks(range(15))\n",
    "axes[0].set_yticklabels(top_positive_words[:15])\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "axes[0].set_title('Top 15 Words for Positive Sentiment', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Negative words\n",
    "axes[1].barh(range(15), top_negative_scores[:15], color='red', alpha=0.7)\n",
    "axes[1].set_yticks(range(15))\n",
    "axes[1].set_yticklabels(top_negative_words[:15])\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Coefficient Value')\n",
    "axes[1].set_title('Top 15 Words for Negative Sentiment', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76871a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction function with detailed output\n",
    "def analyze_review(review_text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a movie review with detailed breakdown\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"SENTIMENT ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nOriginal Review:\\n{review_text}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned = clean_text(review_text)\n",
    "    print(f\"\\nCleaned Review:\\n{cleaned}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    # Transform and predict\n",
    "    text_tfidf = tfidf.transform([cleaned])\n",
    "    prediction = log_reg.predict(text_tfidf)[0]\n",
    "    probabilities = log_reg.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\nPrediction: {prediction.upper()}\")\n",
    "    print(f\"Confidence: {max(probabilities)*100:.2f}%\")\n",
    "    print(f\"\\nProbability Breakdown:\")\n",
    "    print(f\"  Negative: {probabilities[0]*100:.2f}%\")\n",
    "    print(f\"  Positive: {probabilities[1]*100:.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return prediction, probabilities\n",
    "\n",
    "# Test it\n",
    "sample = \"The cinematography was breathtaking and the story kept me engaged throughout!\"\n",
    "analyze_review(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROJECT SUMMARY & KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDataset:\")\n",
    "print(f\"  - Total reviews: {len(df):,}\")\n",
    "print(f\"  - Positive: {len(df[df['sentiment']=='positive']):,}\")\n",
    "print(f\"  - Negative: {len(df[df['sentiment']=='negative']):,}\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  - Logistic Regression: {accuracy_score(y_test, y_pred_log)*100:.2f}%\")\n",
    "print(f\"  - Naive Bayes: {accuracy_score(y_test, y_pred_nb)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  - Both models achieved high accuracy (85%+)\")\n",
    "print(\"  - Logistic Regression slightly outperformed Naive Bayes\")\n",
    "print(\"  - Model shows high confidence (95%+) on clear positive/negative reviews\")\n",
    "print(\"  - Neutral reviews are challenging (classified as negative)\")\n",
    "\n",
    "print(\"\\nTechnical Approach:\")\n",
    "print(\"  - Text preprocessing: Removed HTML, URLs, stopwords\")\n",
    "print(\"  - Feature extraction: TF-IDF with 5,000 features\")\n",
    "print(\"  - Train/Test split: 80/20\")\n",
    "print(\"  - Models: Logistic Regression & Multinomial Naive Bayes\")\n",
    "\n",
    "print(\"\\nPossible Improvements:\")\n",
    "print(\"  - Add more models (Random Forest, SVM, Neural Networks)\")\n",
    "print(\"  - Handle neutral sentiment as separate class\")\n",
    "print(\"  - Use word embeddings (Word2Vec, GloVe)\")\n",
    "print(\"  - Fine-tune hyperparameters\")\n",
    "print(\"  - Try deep learning (LSTM, BERT)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
